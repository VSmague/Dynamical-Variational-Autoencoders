{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1314a287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d74b4d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7d6c010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eceff94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6c97ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchcodec\n",
      "  Downloading torchcodec-0.8.1-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
      "Downloading torchcodec-0.8.1-cp313-cp313-manylinux_2_28_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchcodec\n",
      "Successfully installed torchcodec-0.8.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchcodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11b94a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting soundfile\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\n",
      "Collecting cffi>=1.0 (from soundfile)\n",
      "  Downloading cffi-2.0.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: numpy in /opt/python/lib/python3.13/site-packages (from soundfile) (2.3.5)\n",
      "Collecting pycparser (from cffi>=1.0->soundfile)\n",
      "  Downloading pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n",
      "Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cffi-2.0.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (219 kB)\n",
      "Downloading pycparser-2.23-py3-none-any.whl (118 kB)\n",
      "Installing collected packages: pycparser, cffi, soundfile\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [soundfile]/3\u001b[0m [cffi]\n",
      "\u001b[1A\u001b[2KSuccessfully installed cffi-2.0.0 pycparser-2.23 soundfile-0.13.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install soundfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddc4c73",
   "metadata": {},
   "source": [
    "# I- Conversion des données au format MEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76bab110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import torchaudio\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------\n",
    "# DATASET PYTORCH WAV SANS TORCHAUDIO.LOAD\n",
    "# -----------------------\n",
    "class MelDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, segment_frames=200, sample_rate=22050, n_mels=80):\n",
    "        self.root = root\n",
    "        self.segment_frames = segment_frames\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "        self.mel = MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=1024,\n",
    "            hop_length=256,\n",
    "            win_length=1024,\n",
    "            n_mels=n_mels\n",
    "        )\n",
    "        self.db = AmplitudeToDB()\n",
    "\n",
    "        # lister tous les fichiers wav\n",
    "        self.wav_files = []\n",
    "        for subfolder in os.listdir(root):\n",
    "            sub_path = os.path.join(root, subfolder)\n",
    "            if os.path.isdir(sub_path):\n",
    "                for f in os.listdir(sub_path):\n",
    "                    if f.endswith(\".wav\"):\n",
    "                        self.wav_files.append(os.path.join(sub_path, f))\n",
    "\n",
    "        if len(self.wav_files) == 0:\n",
    "            raise RuntimeError(\"Aucun fichier .wav trouvé dans root.\")\n",
    "\n",
    "        self._compute_statistics()\n",
    "\n",
    "    # -----------------------\n",
    "    # CHARGEMENT AUDIO AVEC SOUNDFILE\n",
    "    # -----------------------\n",
    "    def _load_wav(self, path):\n",
    "        wav, sr = sf.read(path)            # -> numpy array (N,) ou (N,2)\n",
    "        if wav.ndim == 2:                  # stereo → mono\n",
    "            wav = wav.mean(axis=1)\n",
    "        wav = wav.astype(np.float32)\n",
    "        \n",
    "        # conversion tensor\n",
    "        wav = torch.tensor(wav).unsqueeze(0)  # (1,N)\n",
    "\n",
    "        # resample si nécessaire\n",
    "        if sr != self.sample_rate:\n",
    "            wav = torchaudio.functional.resample(wav, sr, self.sample_rate)\n",
    "\n",
    "        # normalisation amplitude\n",
    "        wav = wav / wav.abs().max()\n",
    "\n",
    "        return wav\n",
    "\n",
    "    # conversion en melspec\n",
    "    def _wav_to_mel(self, wav):\n",
    "        mel = self.mel(wav)\n",
    "        mel_db = self.db(mel)\n",
    "        return mel_db.squeeze(0).transpose(0, 1)  # (frames, mels)\n",
    "\n",
    "    # calcul mean/std global\n",
    "    def _compute_statistics(self):\n",
    "        print(\"Calcul des statistiques globales...\")\n",
    "        mel_list = []\n",
    "        for path in self.wav_files:\n",
    "            wav = self._load_wav(path)\n",
    "            mel = self._wav_to_mel(wav)\n",
    "            mel_list.append(mel)\n",
    "\n",
    "        all_mels = torch.cat(mel_list, dim=0)\n",
    "        self.mean = all_mels.mean(dim=0)\n",
    "        self.std = all_mels.std(dim=0)\n",
    "        print(\"done.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wav_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav = self._load_wav(self.wav_files[idx])\n",
    "        mel = self._wav_to_mel(wav)\n",
    "\n",
    "        mel = (mel - self.mean) / (self.std + 1e-6)\n",
    "\n",
    "        if mel.shape[0] < self.segment_frames:\n",
    "            pad = self.segment_frames - mel.shape[0]\n",
    "            mel = F.pad(mel, (0, 0, 0, pad))\n",
    "\n",
    "        max_start = mel.shape[0] - self.segment_frames\n",
    "        start = torch.randint(0, max_start + 1, (1,)).item()\n",
    "        return mel[start:start + self.segment_frames]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9054ff16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcul des statistiques globales...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "dataset = MelDataset(\n",
    "    root=\"/home/onyxia/Dynamical-Variational-Autoencoders/data/data_wav\",\n",
    "    segment_frames=200,\n",
    "    sample_rate=22050,\n",
    "    n_mels=80\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb20e3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2922"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a8d2d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2922/2922 [03:31<00:00, 13.81it/s]\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"/home/onyxia/Dynamical-Variational-Autoencoders/data/mels_saved\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for i in tqdm.tqdm(range(len(dataset))):\n",
    "    mel = dataset[i]   # (200, 80)\n",
    "    torch.save(mel, os.path.join(save_dir, f\"mel_{i}.pt\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd5638b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2922/2922 [03:30<00:00, 13.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sauvegarde terminée.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# dossier global qui contiendra mel_0000, mel_1000, mel_2000, ...\n",
    "root_save = \"/home/onyxia/Dynamical-Variational-Autoencoders/data/mels_saved\"\n",
    "os.makedirs(root_save, exist_ok=True)\n",
    "\n",
    "MAX_FILES_PER_FOLDER = 1000\n",
    "\n",
    "folder_idx = 0\n",
    "file_idx_in_folder = 0\n",
    "\n",
    "current_folder = os.path.join(root_save, f\"part_{folder_idx}\")\n",
    "os.makedirs(current_folder, exist_ok=True)\n",
    "\n",
    "for i in tqdm.tqdm(range(len(dataset))):\n",
    "\n",
    "    # changement de folder\n",
    "    if file_idx_in_folder >= MAX_FILES_PER_FOLDER:\n",
    "        folder_idx += 1\n",
    "        file_idx_in_folder = 0\n",
    "        current_folder = os.path.join(root_save, f\"part_{folder_idx}\")\n",
    "        os.makedirs(current_folder, exist_ok=True)\n",
    "\n",
    "    mel = dataset[i]\n",
    "\n",
    "    save_path = os.path.join(current_folder, f\"mel_{i}.pt\")\n",
    "    torch.save(mel, save_path)\n",
    "\n",
    "    file_idx_in_folder += 1\n",
    "\n",
    "print(\"Sauvegarde terminée.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddcffcf",
   "metadata": {},
   "source": [
    "# II- Ouverture du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fdc4318",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SavedMelDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "\n",
    "        self.files = []\n",
    "\n",
    "        # parcourir part_0, part_1, etc.\n",
    "        for folder in sorted(os.listdir(root)):\n",
    "            p = os.path.join(root, folder)\n",
    "            if os.path.isdir(p):\n",
    "                for f in os.listdir(p):\n",
    "                    if f.endswith(\".pt\"):\n",
    "                        # on stocke le chemin complet\n",
    "                        self.files.append(os.path.join(p, f))\n",
    "\n",
    "        self.files.sort()  # garantir un ordre stable\n",
    "\n",
    "        if len(self.files) == 0:\n",
    "            raise RuntimeError(\"Aucun fichier .pt trouvé dans les folders.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.load(self.files[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3cab58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2922\n",
      "torch.Size([200, 80])\n"
     ]
    }
   ],
   "source": [
    "dataset2 = SavedMelDataset(\n",
    "    \"/home/onyxia/Dynamical-Variational-Autoencoders/data/mels_saved\"\n",
    ")\n",
    "\n",
    "print(len(dataset2))\n",
    "print(dataset2[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666e3f55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
